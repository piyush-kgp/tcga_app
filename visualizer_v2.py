# -*- coding: utf-8 -*-
"""GradCamTCGA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mY6gm8eLleBXe35YcGMW-Hzc4f69dUB_
"""
# !nvidia-smi

# !tar xvzf brca.tar.gz

import torch.nn as nn
import torch.nn.functional as F
# from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, models, transforms
import torch.optim as optim
from torch.utils.data import DataLoader, random_split, Dataset
import torch
import torchvision
from PIL import Image
import argparse
from sklearn.metrics import classification_report
import sklearn.metrics as metrics
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd
from PIL import Image, ImageFile
import json
import matplotlib.pyplot as plt
from vis_utils import preprocess_image,save_class_activation_images
ImageFile.LOAD_TRUNCATED_IMAGES = True


def get_hyperpara(organ):
    organ_parameters = {
        "BRCA": {"optimizer": "RMSprop", "n_units_l0": 87, "lr": 5.943487128044342e-05, "n_layers": 1, "dropout_l0": 0.4494231064473708},
        "COAD": {"n_layers": 2, "n_units_l0": 51, "dropout_l0": 0.3633901781496375, "n_units_l1": 96, "dropout_l1": 0.24938970860378834, "optimizer": "RMSprop", "lr": 9.325098201927569e-05},
        "KICH": {"optimizer": "Adam", "n_layers": 2, "lr": 0.00034174604486655424, "dropout_l1": 0.2590922048730916, "dropout_l0": 0.4508962491601658, "n_units_l0": 85, "n_units_l1": 128},
        "KIRC": {"dropout_l0": 0.23949940999396052, "optimizer": "Adam", "lr": 7.340640278726522e-05, "dropout_l1": 0.23075402550504015, "n_layers": 2, "n_units_l0": 90, "n_units_l1": 60},
        "KIRP": {"optimizer": "Adam", "n_units_l0": 101, "lr": 0.0001821856779144607, "n_units_l1": 127, "dropout_l0": 0.20045233657011846, "n_layers": 2, "dropout_l1": 0.24987265944230833},
        "LIHC": {"lr": 0.000305882784838735, "n_units_l1": 46, "n_layers": 2, "dropout_l1": 0.33086051605996936, "dropout_l0": 0.3992380294683205, "n_units_l0": 30, "optimizer": "Adam"},
        "LUAD": {"dropout_l1": 0.40199979514662076, "lr": 0.022804111060047597, "n_layers": 2, "dropout_l0": 0.2494019459238684, "n_units_l0": 56, "optimizer": "SGD", "n_units_l1": 72},
        "LUSC": {"dropout_l0": 0.36601040402058993, "lr": 0.00012063189382769252, "dropout_l2": 0.23198155500094464, "dropout_l1": 0.46238255107808696, "n_units_l1": 5, "n_units_l0": 128, "optimizer": "Adam", "n_units_l2": 79, "n_layers": 3},
        "PRAD": {"lr": 0.00041534966057817655, "n_layers": 1, "dropout_l0": 0.36193748808143655, "optimizer": "Adam", "n_units_l0": 85},
        "READ": {"n_units_l1": 4, "n_layers": 3, "n_units_l2": 101, "optimizer": "Adam", "lr": 0.00010345081363438437, "dropout_l1": 0.32566024092209167, "n_units_l0": 114, "dropout_l0": 0.3736031169357898, "dropout_l2": 0.4320328109269479},
        "STAD": {"n_layers": 1, "lr": 1.931750132804336e-05, "dropout_l0": 0.4365385207030711, "n_units_l0": 92, "optimizer": "Adam"}
    }
    dropouts = []
    hidden_layer_units = []
    for i in range(organ_parameters[organ]['n_layers']):
        dropouts.append(organ_parameters[organ]['dropout_l{}'.format(i)])
        hidden_layer_units.append(organ_parameters[organ]['n_units_l{}'.format(i)])
    optimizer = organ_parameters[organ]['optimizer']
    lr = organ_parameters[organ]['lr']
    return dropouts,hidden_layer_units,optimizer,lr

def define_model(dropouts,hidden_layer_units,num_classes=2):
    # We optimize the number of layers, hidden untis and dropout ratio in each layer.
    layers = []
    model = models.resnet18(pretrained=True)
    in_features = model.fc.in_features
    for i in range(len(dropouts)):
        out_features = hidden_layer_units[i]
        layers.append(nn.Linear(in_features, out_features))
        layers.append(nn.ReLU())
        p = dropouts[i]
        layers.append(nn.Dropout(p))
        in_features = out_features
    layers.append(nn.Linear(in_features, num_classes))
    model.fc = nn.Sequential(*layers)
    return model




class CamExtractor():
    """
        Extracts cam features from the model
    """
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None

    def save_gradient(self, grad):
        self.gradients = grad

    def forward_pass_on_convolutions(self, x):
        """
            Does a forward pass on convolutions, hooks the function at given layer
        """
        conv_output = None
        for module_pos, module in self.model._modules.items():
            try:
                x = module(x)  # Forward
            except:
                x = x.view(-1)
                x = module(x)
            if module_pos == self.target_layer:
                x.register_hook(self.save_gradient)
                conv_output = x  # Save the convolution output on that layer
        return conv_output, x


    def forward_pass(self, x):
        """
            Does a full forward pass on the model
        """
        # Forward pass on the convolutions
        conv_output, x = self.forward_pass_on_convolutions(x)
        return conv_output, x

class GradCam():
    """
        Produces class activation map
    """
    def __init__(self, model,target_layer):
        self.model = model
        self.model.eval()
        # Define extractor
        self.extractor = CamExtractor(self.model, target_layer)

    def generate_cam(self, original_image, input_tensor, heatmap_fname, heatmap_superimposed_fname):

        # Full forward pass
        # conv_output is the output of convolutions at specified layer
        # model_output is the final output of the model (1, 1000)
        conv_output, model_output = self.extractor.forward_pass(input_tensor)
        model_output = model_output.view(1,-1)
        target_class = np.argmax(model_output.data.numpy())
        # Target for backprop
        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()
        one_hot_output[0][target_class] = 1
        # Zero grads
        self.model.zero_grad()
        # Backward pass with specified target
        model_output.backward(gradient=one_hot_output, retain_graph=True)
        # Get hooked gradients
        guided_gradients = self.extractor.gradients.data.numpy()[0]
        # Get convolution outputs
        target = conv_output.data.numpy()[0]
        # Get weights from gradients
        weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient
        # Create empty numpy array for cam
        cam = np.ones(target.shape[1:], dtype=np.float32)
        # Multiply each weight with its conv output and then, sum
        for i, w in enumerate(weights):
            cam += w * target[i, :, :]
        cam = np.maximum(cam, 0)
        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1
        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize
        cam = np.uint8(Image.fromarray(cam).resize((input_tensor.shape[2],
                       input_tensor.shape[3]), Image.ANTIALIAS))/255
        # ^ I am extremely unhappy with this line. Originally resizing was done in cv2 which
        # supports resizing numpy matrices with antialiasing, however,
        # when I moved the repository to PIL, this option was out of the window.
        # So, in order to use resizing with ANTIALIAS feature of PIL,
        # I briefly convert matrix to PIL image and then back.
        # If there is a more beautiful way, do not hesitate to send a PR.

        # You can also use the code below instead of the code line above, suggested by @ ptschandl
        # from scipy.ndimage.interpolation import zoom
        # cam = zoom(cam, np.array(input_image[0].shape[1:])/np.array(cam.shape))
        save_class_activation_images(original_image, cam, heatmap_fname, heatmap_superimposed_fname)


        return target_class



def get_output(fp, heatmap_fname, heatmap_superimposed_fname, organ):

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("DEVICE {}".format(device), flush=True)

    model_checkpoint = "CheckPoints/{}_best_model.pth".format(organ)
    ckpt = torch.load(model_checkpoint) if device=="cuda" else \
           torch.load(model_checkpoint, map_location=torch.device('cpu'))
    ckpt = {k.replace("module.", ""): v for k, v in ckpt.items()}

    dropouts,hidden_layer_units,optimizer_name,lr = get_hyperpara(organ)
    model = define_model(dropouts,hidden_layer_units,num_classes=2)
    model.load_state_dict(ckpt)

    model = model.to(device)

    data_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.596, 0.436, 0.586], [0.2066, 0.240, 0.186])
        ])

    img = Image.open(fp)
    X = data_transform(img)
    X = X.reshape(-1,3,224,224)

    model.eval()
    grad_cam = GradCam(model, target_layer="layer4")
    pred = grad_cam.generate_cam(img.convert('RGB'), X, heatmap_fname, heatmap_superimposed_fname)
    return pred
